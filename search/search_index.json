{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Project","text":"<p>Info</p> <p>This website provides technical instructions how to set up and reuse the Odelia technical platform for other use cases. The official Odelia website, which introduces the consortium, can be found here: odelia.ai</p>"},{"location":"#mission","title":"Mission","text":"<p>ODELIA\u2019s mission is to develop and implement a pan-European swarm learning network that enables privacy-preserving and democratic training of medical AI algorithms. By focusing on breast cancer detection in MRI screenings, the project aims to demonstrate the power of swarm learning and its potential applications in various clinical settings.</p> <p>Prof. Jakob N. Kather, Scientific Coordinator, EKFZ for Digital Health</p> <p>Swarm Learning has the potential to address the challenges of data collection and providing a framework for collaboration in AI training and ultimately improve the quality of healthcare for patients in Europe.</p>"},{"location":"#vision-for-the-future","title":"Vision for the Future","text":"<p>ODELIA envisions a future where swarm learning becomes a standard practice in the development of medical AI models, ensuring data privacy and fostering collaboration among healthcare institutions. The project aspires to pave the way for a new era of AI-driven medical advancements that empower healthcare providers and improve patient outcomes across Europe and beyond.</p>"},{"location":"#project-context","title":"Project Context","text":"<p>In a rapidly evolving healthcare landscape, the need for advanced AI-driven solutions and secure data sharing has become increasingly important. ODELIA tackles these challenges by harnessing the power of swarm learning, providing a decentralized, privacy-preserving approach to AI model training. By addressing the limitations of traditional AI training methods, ODELIA paves the way for improved diagnostic accuracy and enhanced collaboration among healthcare providers. The project focuses on the critical area of breast cancer detection in MRI screenings, demonstrating the real-world impact of swarm learning on patient outcomes and advancing the field of medical AI.</p> <p>Dr. Daniel Truhn, Scientific Coordinator, RWTH Aachen</p> <p>ODELIA is a pioneering project that brings together the best of AI, medical imaging, and data privacy. I am proud to be part of this collaborative effort to transform healthcare. As a radiologist, I believe our swarm learning approach will revolutionize medical AI and set new standards in privacy-preserving AI development to advance healthcare.</p>"},{"location":"#partner","title":"Partner","text":"<p>The ODELIA Consortium brings together a diverse and talented team of experts from across Europe. Comprising 12 partners from 8 countries, our consortium represents a multidisciplinary collaboration of specialists in the fields of medicine, artificial intelligence, and big data: European Institute for Biomedical Imaging Research (Austria), University Hospital Aachen (Germany), Vall d\u2019Hebron Institute of Oncology (Spain), Mitera Hospita (Greece), Radboud University Medical Center (Netherlands), University Medical Center Utrecht (Netherlands), Ribera Salud (Spain), Fraunhofer Institute for Digital Medicine MEVIS (Germany), OSIMIS (Belgium), Technische Universit\u00e4t Dresden (Germany), University of Zurich (Switzerland) and University of Cambridge (United Kingdom).</p> <p></p>"},{"location":"#funding","title":"Funding","text":"<p>This project has received funding from the European Union\u2019s Horizon Europe research and innovation programme under grant agreement No 101057091.</p>"},{"location":"background/","title":"Background","text":"<p>Swarm Learning</p> <p>Swarm learning is a decentralized machine learning solution that uses edge computing and blockchain technology to enable peer-to-peer collaboration. For this project, we are using HPE Swarm Learning, which was developed by Hewlett Packard Enterprise.</p> <p>Swarm Learning Course</p> <p>You can find a free Swarm Leaning Course explained in a generally understandable way under https://learn.software.hpe.com/swarm-learning-essentials. </p>"},{"location":"background/#general","title":"General","text":""},{"location":"background/#what-is-swarm-learning","title":"What is Swarm Learning?","text":"<p>Swarm learning is a decentralized machine learning solution that uses edge computing and blockchain technology to enable peer-to-peer collaboration. It allows multiple collaborators to share data insights without sharing the data itself, protecting data privacy and security while allowing all contributors to benefit from collective learnings.</p>"},{"location":"background/#why-is-swarm-learning-important","title":"Why is swarm learning important?","text":"<p>As more data is collected and processed at the intelligent edge, its true value can only be realized by sharing it and turning it into a collective understanding. However, sharing data like this introduces security risks and in some cases is prohibited by government regulations. Because swarm learning shares insights rather than source data, the learnings derived from protected data can be safely shared across locations and even across organizations.</p> <p>Swarm learning can play a vital role in improving the accuracy of artificial intelligence (AI) models. When organizations only have access to their own data, their AI models will evolve based solely on information about those individuals with whom the organization has previously or is currently engaged, creating bias in the models. With swarm learning, an organization can combine its proprietary data with the learnings from other organizations, increasing accuracy and reducing bias.</p>"},{"location":"background/#what-are-the-benefits-of-swarm-learning","title":"What are the benefits of swarm learning?","text":"<p>Today, the huge volumes of data generated and collected at different edge locations creates a challenge for a traditional, centralized machine learning approach. These algorithms need data to be in a consolidated location, but moving large amounts of data from multiple sources to a single location introduces security risks and latency concerns. Swarm learning\u2019s decentralized approach allows data to be applied to AI models closer to the source, with only the learnings being moved. Blockchain technology enables multiple edge locations, collectively called a swarm, to share insights with one another in a trusted manner and prevents bad actors from gaining unauthorized access to the swarm. This decentralized approach allows models to generate answers more quickly and organizations to have greater opportunities for shared learning, even outside their own four walls. At the same time, the privacy of source data is protected, because data movement is limited. This also reduces data sprawl, as data does not need to be duplicated to a core or central location. By training models and harnessing insights at the edge, organizations can make decisions more quickly, where they are most relevant, resulting in better outcomes. Dataset sizes available to models can increase, making them more reliable and less prone to biases. At the same time, data governance and privacy are preserved.</p> <p>Explanation Video</p> <p>You can find an explanation video here: HPE Swarm Learning Product Introduction</p> <p><sup>1</sup></p>"},{"location":"background/#swarm-learning-architecture","title":"Swarm Learning Architecture","text":"<p>Swarm Learning is made up of various components known as nodes, such as Swarm Learning (SL) nodes, Swarm Network (SN) nodes, Swarm Learning Command Interface (SWCI) nodes, and Swarm Operator (SWOP) nodes. Each node of SL is modularized and runs in a separate container.</p> <p>Blockchain</p> <p>Only metadata is written to the blockchain. The model itself is not stored in the blockchain.</p> <p><sup>2</sup></p>"},{"location":"background/#swarm-learning-sl-node","title":"Swarm Learning (SL) node","text":"<p>run the core of Swarm Learning. An SL node works in collaboration with all the other SL nodes in the network. It regularly shares its learnings with the other nodes and incorporates their insights. SL nodes act as an interface between the user model application and other Swarm Learning components. SL nodes take care of distributing and merging model weights in a secured way.</p>"},{"location":"background/#swarm-network-sn-node","title":"Swarm Network (SN) node","text":"<p>form the blockchain network. The current version of Swarm Learning uses an open-source version of Ethereum as the underlying blockchain platform. The SN nodes interact with each other using this blockchain platform to maintain and track progress. The SN nodes use this state and progress information to coordinate the working of the other swarm learning components.</p> <p>Sentinel Node is a special SN node. The Sentinel node is responsible for initializing the blockchain network. This is the first node to start.</p>"},{"location":"background/#swarm-learning-command-interface-swci-node","title":"Swarm Learning Command Interface (SWCI) node","text":"<p>is the command interface tool to the Swarm Learning framework. It is used to monitor the Swarm Learning framework. SWCI nodes can connect to any of the SN nodes in a given Swarm Learning framework to manage the framework.</p>"},{"location":"background/#swarm-operator-swop-node","title":"Swarm Operator (SWOP) node","text":"<p>is an agent that can manage Swarm Learning operations. SWOP is responsible to execute tasks that are assigned to it. A SWOP node can execute only one task at a time. SWOP helps in executing tasks such as starting and stopping Swarm runs, building and upgrading ML containers, and sharing models for training.</p>"},{"location":"background/#license-server","title":"License Server","text":"<p>installs and manages the license that is required to run the Swarm Learning framework. The licenses are managed by the AutoPass License Server (APLS) that runs on a separate node.</p>"},{"location":"background/#swarm-learning-component-interactions","title":"Swarm Learning Component Interactions","text":"Callout Description 1 (SN Peer-to-Peer Port) This port is used by each SN node to share blockchain internal state information with the other SN nodes. The default value of this port is 30303. 2 (SN API Port) This API server is used by the SL nodes to send and receive state information from the SN node that they are registered with. It is also used by SWCI and SWOP nodes to manage and view the status of the Swarm Learning framework. The default value of this port is 30304. 3 (SL File Server Port) This port is used by each SL node to run a file server. This file server is used to share insights learned from training the model with the other SL nodes in the network. The default value of this port is 30305. 4 (License Server API Port) This port is used by the License Server node to run a REST-based API server and a management interface. The API server is used by the SN, SL, SWOP, and SWCI nodes to connect to the License Server and acquire licenses. The management interface is used by Swarm Learning platform administrators to connect to the License Server from browsers and administer licenses. The default value of this port is 5814. 5 (SWCI API server port) This port is used by the SWCI node to optionally run a REST-based API service. This SWCI API service can be used to control and manage the Swarm Learning framework from a program by using the library provided in the wheels package. The default value of this port is 30306. 6 (SL_REQUEST_CHANNEL and SL_RESPONSE_CHANNEL) These named pipes (FIFO) are used between each pair of ML and SL nodes for exchanging the model parameters. <p><sup>3</sup></p>"},{"location":"background/#swarm-learning-concepts","title":"Swarm Learning Concepts","text":"<p>This section provides information about key Swarm Learning concepts. The subsequent section describes working of a Swarm Learning node, how to Swarm enable an ML algorithm, and interactions with SL, contexts, training contract, Taskrunner contract, and task</p> <p>SWCI is the command interface tool to the Swarm Learning framework. It is used to view the status, control, and manage the Swarm Learning framework. SWCI manages the Swarm Learning framework using contexts and contracts. Learn more here.</p> <p>An SWCI context is a string identifier. It identifies an SWCI command environment and has artifacts (API server IP, port, environment variables, and versions) that are used to execute SWCI commands. SWCI can have only one active context at any given time even if multiple contexts are created.</p> <p>Swarm Learning training contracts are used to control the swarm learning training process. It is an instance of Ethereum smart contract. It is deployed into the blockchain and registered into Swarm Learning Network using the <code>CREATE CONTRACT</code> command.</p> <p>Smart Contract</p> <p>When a contract is created, it is permanent and cannot be deleted.</p> <p>SWOP is the central component of the Taskrunner framework. It is packaged as a SWOP container. Taskrunner framework is a decentralized task management framework. Learn more here.</p> <p>A Task is a well-defined unit of work, that can be assigned to a Taskrunner. Tasks are instantiated according to the schema specified in the Task Schema YAML file.</p> <p>A Taskrunner is an instance of Ethereum smart contract used to coordinate execution of task by SWOP nodes.</p> <p><sup>4</sup></p>"},{"location":"background/#working-of-a-swarm-learning-node","title":"Working of a Swarm Learning Node","text":"<ol> <li>SL node starts by acquiring a digital identity, which can be either user-provided CERTS or SPIRE certificates.</li> <li>SL node acquires a license to run.</li> <li>SL node registers itself with an SN node.</li> <li>SL node starts a file server and announces to the SN node that it is ready to run the training program.</li> <li>Waits for the User ML component to start the ML model training.</li> </ol> <p>SL node works in collaboration with all the other SL nodes in the network. It regularly shares its learnings with the other nodes and incorporates their insights. Users can control the periodicity of this sharing by defining a Synchronization Frequency (from now on referred to as, sync frequency.) This frequency specifies the number of training batches after which the nodes share their learnings.</p> <p>Sync Frequency</p> <p>Specifying a large value reduces the rate of synchronization and a small value increases it. Frequent synchronization slows down the training process while infrequent ones may reduce the accuracy of the final model. Therefore, the sync frequency must be treated as a hyperparameter and chosen with some caution.</p> <p>Swarm Learning can automatically control the sync frequency, this feature is called Adaptive Synchronization Frequency. This feature judges the training progress by monitoring the mean loss. A reduction in the mean loss infers that the training is progressing well. As a response, it increases the sync frequency and enables more batches to run before sharing the learnings. This makes the training run faster. In contrast, when the loss does not improve, Adaptive Sync frequency reduces the sync frequency and synchronizes the models more frequently.</p> <p>At the end of every sync frequency, when it is time to share the learning from the individual model, one of the SL nodes is designated as \u201cleader\u201d. This leader node collects the individual models from each peer node and merges them into a single model by combining parameters of all the individuals.</p> <p>Merge Methods are one of the core operations in Swarm Learning that ensures learning is shared across SL nodes. Swarm Learning provides three different merge options via Swarm callback. The merge options are 'mean', 'coordmedian' and 'geomedian'. User can pass one of these options via <code>mergeMethod</code> parameter in the Swarm callback. This is an optional parameter for the callback. These options will consider the weightages of individual SL nodes as well. All nodes that have participated in the sync round will receive aggregated model parameters for the next sync round.</p> <p>The 'mean' option aggregates intermediate model parameters using a weighted mean method (also known as weighted federative averaging). This method sums up all the intermediate model parameters in an iterative way and at the end of the merge method, it divides with sum of weightages. This is the default merge method in Swarm.</p> <p>The 'coordmedian' option finds the weighted coordinate median of the intermediate model parameters. These parameters from all the nodes that are participating in the given merge round will get transformed and transposed before calculating the coordinate median. This merge method finds the 50th percentile and tries to converge along with the model parameters over the sync cycles.</p> <p>The 'geomedian' option finds the weighted geometric median of the intermediate model parameters. It uses Weiszfeld's algorithm which is an iterative method to calculate the geometric median. It starts with model parameters average as an initial estimate and iterate to find a better estimate. This process is repeated until the difference between new estimate and old estimate reaches to a threshold value.</p> <p>Leader Failure Detection and Recovery (LFDR) feature enables SL nodes to continue Swarm training during merging process when an SL leader node fails. A new SL leader node is selected to continue the merging process. If the failed SL leader node comes back after the new SL leader node is in action, the failed SL leader node is treated as a normal SL node and contributes its learning to the swarm global model.</p> <p>Each peer SL node then uses this merged model to start the next training batch. This process is coordinated by the SN network. The models are exchanged using the Swarm Learning file server.</p> <p>A Swarm Learning ML program can specify a Minimum Number of Peers that are required to perform the synchronization. If the actual number of peers is less than this threshold value, the platform blocks the synchronization process until the required number of peers becomes available and reaches the synchronization point.</p> <p><sup>5</sup></p>"},{"location":"background/#adapting-an-ml-algorithm-for-swarm-learning","title":"Adapting an ML Algorithm for Swarm Learning","text":"<p>you can transform any Keras (with TensorFlow 2 backend) or PyTorch based ML program that has been written using Python3 into a Swarm Learning ML program by making a few simple changes to the model training code by including the SwarmCallback API object. See the examples included with the Swarm Learning package for a sample code.</p> <p>Your ML program algorithm can be any parametrized supervised learning model. It can be fully trainable model or a partially trainable model when using transfer learning. Learn more here.</p> <p><sup>6</sup></p>"},{"location":"background/#whitepaper","title":"Whitepaper","text":"<p>You can find the whitepaper here: Swarm Learning: Turn your distributed data into competitive edge</p> <p>All information on this page has been gathered from various sources provided by Hewlett Packard Enterprise.</p> <ol> <li> <p>https://www.hpe.com/us/en/what-is/swarm-learning.html\u00a0\u21a9</p> </li> <li> <p>https://support.hpe.com/hpesc/public/docDisplay?docId=sd00001420en_us&amp;page=GUID-5924CFDA-389E-40ED-94C1-543FEDDFE872.html\u00a0\u21a9</p> </li> <li> <p>https://support.hpe.com/hpesc/public/docDisplay?docId=sd00001420en_us&amp;page=GUID-CE2496F4-22BD-468B-AD40-011E3F113E6E.html\u00a0\u21a9</p> </li> <li> <p>https://support.hpe.com/hpesc/public/docDisplay?docId=sd00001420en_us&amp;page=GUID-01AF4513-24A0-49DA-B345-E28A054E87B8.html\u00a0\u21a9</p> </li> <li> <p>https://support.hpe.com/hpesc/public/docDisplay?docId=sd00001420en_us&amp;page=GUID-80EB950E-DB95-469F-A3DF-E14BBD005486.html\u00a0\u21a9</p> </li> <li> <p>https://support.hpe.com/hpesc/public/docDisplay?docId=sd00001420en_us&amp;page=GUID-06917027-B029-400D-B9B0-9D2D5A9F29DB.html\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/","title":"Getting started","text":"<p>HPE Swarm Learning</p> <p>This project uses HPE Swarm Learning. For general documentation see: https://github.com/HewlettPackard/swarm-learning</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>HPE Swarm Learning can run on any hardware that supports executing container software (Docker).</p>"},{"location":"getting-started/#hardware","title":"Hardware","text":"<ul> <li>Any x86-64 hardware</li> <li>System memory of 32 GB or more</li> <li>Hard disk space of 200 GB or more</li> <li>Qualified with HPE Edgeline, Proliant DL380, and Apollo 6500</li> </ul> <p>Note</p> <p>HPE Swarm Learning can be deployed with Nvidia GPUs (accelerator cards) and AMD GPUS (accelerator cards).</p>"},{"location":"getting-started/#supported-operating-systems-and-platforms","title":"Supported Operating Systems and Platforms","text":"<p>HPE recommends that you run each Swarm Network node, and Swarm Learning node on dedicated systems to get the best performance from the platform. The recommended requirements for each system are as follows:</p> <p>Note</p> <p>The requirements of system running the user ML node is driven by the complexity of the ML algorithm. GPUs may also be needed.</p>"},{"location":"getting-started/#network","title":"Network","text":"<ul> <li>A minimum of one or a maximum four open TCP/IP ports in each node. All swarm nodes must be able to access the ports of every other node. </li> <li>Stable internet connectivity to download Swarm Learning package and Docker images.</li> </ul> <p>Expoded ports</p> <p>Depending on the type of Swarm Learning components that are running on a host, some or all these ports must be opened to allow the Swarm Learning containers to communicate with each other:</p> <ul> <li> <p>A Swarm Network peer-to-peer port on the hosts running Swarm Network nodes. By default, port 30303 is used.</p> </li> <li> <p>A Swarm Network API server port on the hosts running Swarm Network nodes. By default, port 30304 is used.</p> </li> <li> <p>Swarm Learning file server port on the hosts running Swarm Learning nodes. By default, port 30305 is used.</p> </li> <li> <p>A License Server API port on the host running the License Server. By default, port 5814 is used.</p> </li> <li> <p>(Optional). An SWCI API server port that is used by the SWCI node to run a REST based API service. By default, port 30306 is used.</p> </li> </ul>"},{"location":"getting-started/#operating-systems","title":"Operating Systems","text":"<ul> <li>Linux - Qualified on Ubuntu 22.04 RHEL 8.5. and SLES 15.0</li> <li>For Swarm Web UI installer, any x86-64 hardware running Linux, Windows, or Mac.</li> </ul>"},{"location":"getting-started/#container-hosting-platform","title":"Container Hosting Platform","text":"<ul> <li>HPE Swarm Learning is qualified with Docker 20.10.5.</li> <li>Configure Docker to run as a non-root user.</li> <li>Configure network proxy settings for Docker.</li> <li>Configure Docker to use IPv4.</li> </ul>"},{"location":"getting-started/#machine-learning-framework","title":"Machine Learning Framework","text":"<ul> <li>Qualified with Keras (TensorFlow 2 backend) and PyTorch 1.5 based Machine Learning models implemented using Python3.</li> </ul>"},{"location":"getting-started/#multi-system-cluster-requirements","title":"Multi System Cluster Requirements","text":"<ul> <li>Synchronized time across all systems using NTP.</li> </ul>"},{"location":"getting-started/#system-preparation","title":"System preparation","text":"<p>Operating system</p> <p>All the following instructions are only tested on Ubuntu.</p> <ol> <li> <p>Update the system with <code>Software Updater</code> or in via <code>terminal</code> with:     <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt upgrade\n</code></pre></p> </li> <li> <p>Install the Nvidia driver via <code>Software Updater \u2192 Settings \u2192 Additonal Drivers \u2192 NVIDIA driver metapackage from nvidia-driver-525 (proprietary) -&gt; Apply Changes</code>.</p> </li> <li> <p>Restart the System.</p> </li> <li> <p>Test the successful driver installation with:     <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Install SSH with:     <pre><code>sudo apt install openssh-client openssh-server\n</code></pre></p> </li> <li> <p>Install Git with:     <pre><code>sudo apt install git\n</code></pre></p> </li> <li> <p>Install Curl with:     <pre><code>sudo apt install curl\n</code></pre></p> </li> <li> <p>Install Docker with:     <pre><code>curl -fsSL get.docker.com | sudo sh\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#creation-of-swarm-user-and-download-of-the-repository","title":"Creation of swarm user and Download of the repository","text":"<ol> <li> <p>Create a user named <code>swarm</code> and add it to the sudoers group:     <pre><code>sudo adduser swarm\n</code></pre> <pre><code>sudo usermod -aG sudo swarm\n</code></pre></p> </li> <li> <p>Login with <code>swarm</code>:     <pre><code>su - swarm\n</code></pre></p> </li> <li> <p>Run the following commands to download the repository:     <pre><code>cd / &amp;&amp; sudo mkdir opt/hpe &amp;&amp; cd opt/hpe &amp;&amp; sudo chmod 777 -R /opt/hpe\n</code></pre> <pre><code>git clone https://github.com/KatherLab/swarm-learning-hpe.git &amp;&amp; cd swarm-learning-hpe\n</code></pre> <pre><code>sudo chmod 777 -R /opt/hpe\n</code></pre></p> </li> </ol>"},{"location":"getting-started/#installing-the-license-server","title":"Installing the License Server","text":"<p>Warning</p> <p>The license server must be provided by only one network member (Sentinel Node).</p> <ol> <li> <p>Go to MY HPE SOFTWARE CENTER.</p> </li> <li> <p>If you have the HPE Passport account, enter the credentials and Sign In. If you do not have it, create the HPE Passport Account and Sign In.</p> </li> <li> <p>After signing in, click Software from the left pane.</p> </li> <li> <p>Locate Search and select Product Info from the Search Type dropdown, and search for Swarm Learning. Search results list the available Swarm Learning products.</p> </li> <li> <p>\"HPE Swarm Learning Community edition\" is the evaluation version that you need to download. Click Action and select Product Details to view the Swarm Learning product details.</p> </li> <li> <p>Click Installation tab to view the APLS download link. Click the link here to view the APLS software downloads. Scroll down and view search results.</p> </li> <li> <p>Download AutoPass License Server. To download, click Action and select Get downloads.</p> </li> <li> <p>Click Download to copy the APLS software (apls-xx.xx.xx.zip) to your system.</p> </li> <li> <p>Extract zip file in Downloads.</p> </li> <li> <p>Execute setup script: <pre><code>cd Downloads/apls-X.x/UNIX\nchmod a+x setup.bin\nsudo ./setup.bin\n</code></pre></p> </li> </ol> <p>Info</p> <p>When it is successfully executed the following appears: <pre><code>Pre-Installation Summary\n------------------------\n\nPlease Review the Following Before Continuing:\n\nProduct Name:\n    HPE AutoPass License Server\n\nInstall Folder:\n    /opt/HP/HP AutoPass License Server\n\nLink Folder:\n    /usr/bin\n\nJava VM Installation Folder:\n    /opt/HP/HP AutoPass License Server/jre\n\nData Folder Directory\n    /var/opt/HP/HP AutoPass License Server\n\nProduct Version\n    9.12.0.0\n\nDisk Space Information (for Installation Target): \n    Required:      304.12 MegaBytes\n    Available: 420,910.62 MegaBytes\n</code></pre> <pre><code>Congratulations HPE AutoPass License Server 9.12.0.0 has been successfully \ninstalled to:\n\n/opt/HP/HP AutoPass License Server\n\nHPE AutoPass License Server GUI can be accessed at :\nhttps://&lt;Host/IP address&gt;:5814/autopass\n\nHPE AutoPass License Server Service Usage:\nhpLicenseServer {start|stop|restart|status}\n</code></pre></p> <ol> <li>visit: https://localhost:5000</li> </ol> <p>Info</p> <p>username: admin</p> <p>password: password</p> <p>If the service is not working </p> <pre><code>cd \"/opt/HP/HP AutoPass License Server/HP AutoPass License Server/HP AutoPass License Server/conf\"\nsudo nano server.xml\n</code></pre> <p>Search with Strg + W for \u201c5814\u201d and replace it with \u201c5000\u201d and save.</p> <pre><code>cd \"/opt/HP/HP AutoPass License Server/HP AutoPass License Server/HP AutoPass License Server/bin\"\nsudo cp hpLicenseServer  /etc/init.d/hpLicenseServer\nsudo chmod 755 /etc/init.d/hpLicenseServer\ncd /etc/init.d\nsudo update-rc.d hpLicenseServer defaults 97 03\nservice hpLicenseServer start\nservice hpLicenseServer status\n</code></pre> <ol> <li>In the APLS web GUI, go to\u00a0<code>License Management\u00a0-&gt;\u00a0Install License</code>\u00a0and note down the lock code.</li> </ol> <p>Info</p> <p>Lock Code = Serial Number</p> <p></p> <ol> <li> <p>Navigate to the\u00a0MY HPE SOFTWARE CENTER\u00a0home page. After signing in with your HPE Passport credentials and perform the following actions:</p> <p>Click\u00a0Software\u00a0(left pane) -&gt; Under\u00a0Search\u00a0Select \"Product Info\" -&gt; enter the string \"Swarm Learning\".</p> <p>Under the search results, For the product \"HPE-SWARM-CMT x.x.x\"-&gt; Click on\u00a0Action\u00a0-&gt;\u00a0Get License</p> </li> <li> <p>Enter the lock code (Serial Number) you got from the\u00a0Install Licenses\u00a0page in the HPE Serial Number field and click\u00a0Activate.</p> </li> <li>Once you activate the licenses, you will see the\u00a0Download Files\u00a0page.</li> <li>Select and download the\u00a0keys and all the listed software files\u00a0(7 files).</li> <li>Install and manage the Swarm Learning license:<ol> <li>Open the APLS management console.</li> <li>Select\u00a0License Management\u00a0-&gt;\u00a0Install License.</li> <li>Select\u00a0Choose\u00a0file to upload the license file that you downloaded and click\u00a0Next.</li> <li>Select the required feature IDs and click\u00a0Install Licenses.</li> </ol> </li> </ol> <p></p> <p>Bugs and Problems</p> <p>Did you find a bug in the code or other problems? Then raise an issue in our Github repository: https://github.com/KatherLab/swarm-learning-hpe/issues</p> <p>In case of problems or requests for improvement of the documentation, please raise an issue at: https://github.com/odelia-ai/odelia-ai.github.io/issues</p>"},{"location":"imprint-privacy-policy/","title":"Imprint and Privacy Policy","text":""},{"location":"imprint-privacy-policy/#imprint","title":"Imprint","text":"<pre><code>EIBIR gemeinn\u00fctzige GmbH\nAm Gestade\n1010 Vienna\nAustria\n\nWebsite: https://odelia.ai\nEmail: gro.ribie@ekebedrogp\nPhone number: +4315334064323\n</code></pre>"},{"location":"imprint-privacy-policy/#privacy-policy","title":"Privacy Policy","text":"<p>In the following, we would like to explain how your data is processed by us.</p> <p>Responsible within the meaning of the GDPR is:</p> <pre><code>EIBIR gemeinn\u00fctzige GmbH\nAm Gestade\n1010 Vienna\nAustria\n\nWebsite: https://odelia.ai\nEmail: pgordebeke@eibir.org\nPhone number: +4315334064323\n</code></pre> <p>If you contact us by e-mail, the data you provide will be stored by us in order to process your request.</p> <p>We will delete the data as soon as storage is no longer necessary or restrict processing if there are statutory retention obligations.</p>"},{"location":"imprint-privacy-policy/#rights-of-data-subjects","title":"Rights of data subjects","text":"<p>As a data subject, you have the right to information, the right to rectification or erasure, the right to restriction of processing and the right to object to the processing of your data. If you have given us your consent, you can revoke it at any time with effect for the future. You also have the right to data portability.</p>"},{"location":"imprint-privacy-policy/#cookies","title":"Cookies","text":"<p>We use cookies. Cookies are small text files that are stored on your end device when you access the site. They cannot transfer viruses or malware to your computer, but they do contain information that enables the user to be identified.</p> <p>A distinction must be made between transient cookies, which are deleted as soon as your browser is closed, and persistent cookies, which are stored beyond the respective session and recognize you the next time you visit the website.</p> <p>In terms of function, a distinction must be made between technically necessary and non-essential cookies.</p> <p>Technically necessary cookies Here you will find all cookies that are required for the operation of our website and its functions (technically necessary cookies). These are usually set in response to an action you have taken. These include registration, login or settings such as language or cookie preferences. It is possible to deactivate these cookies in the browser. In this case, we can no longer guarantee that our website will function properly.</p> <p>Cookies that are not technically necessary Here you will find all cookies that are not absolutely necessary for the operation of our website and its functions (technically unnecessary cookies). The use of such cookies constitutes data processing that is only permitted with your active consent (Art. 6 para. 1 sentence 1 lit. a GDPR). This also applies to the transfer of your personal data to third parties.</p> <ul> <li>ga: This cookie is used by Google Analytics to distinguish unique users by assigning a randomly generated number as a client identifier. It is included in each page request in a site and used to calculate visitor, session and campaign data for the site's analytics reports.     </li> <li>_ga_MVCEFS3QLE: This cookie is used by Google Analytics to distinguish unique users by assigning a randomly generated number as a client identifier. It is included in each page request in a site and used to calculate visitor, session and campaign data for the site's analytics reports. </li> </ul> <p>You can delete individual or all cookies via your browser settings. You also have the option of generally deactivating cookies or restricting them to certain domains via your browser settings.</p>"},{"location":"imprint-privacy-policy/#hosting","title":"Hosting","text":"<p>Our host collects the following data transmitted by your browser in log files:</p> <p>IP address, the address of the previously visited website (referrer request header), date and time of the request, time zone difference to Greenwich Mean Time, content of the request, HTTP status code, amount of data transferred, website from which the request comes and information about the browser and operating system.</p> <p>This is necessary to display our website and to ensure stability and security. This corresponds to our legitimate interest within the meaning of Art. 6 para. 1 sentence 1 lit. f GDPR.</p> <p>There is no tracking and we do not have direct access to this data, but only receive an anonymized, statistical summary. This includes the address of the previously visited page, the frequency of each page accessed and the number of unique visitors. We do not merge this data with other data.</p> <p>We use the following hoster for the provision of our website:</p> <pre><code>GitHub Inc.\n88 Colin P Kelly Jr St\nSan Francisco, CA 94107\nUnited States\n</code></pre> <p>This is the recipient of your personal data. This corresponds to our legitimate interest within the meaning of Art. 6 para. 1 sentence 1 lit. f GDPR in not having to maintain a server on our premises ourselves. Server location is USA.</p> <p>Further information on objection and removal options vis-\u00e0-vis GitHub can be found at: https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement#github-pages</p> <p>You have the right to object to the processing. Whether the objection is successful must be determined as part of a balancing of interests.</p> <p>The data will be deleted as soon as the purpose of the processing no longer applies.</p> <p>The processing of the data specified in this section is not required by law or contract. The functionality of the website is not guaranteed without the processing.</p> <p>GitHub has implemented compliance measures for international data transfers. These apply to all global activities where GitHub processes personal data of natural persons in the EU. These measures are based on the EU Standard Contractual Clauses (SCCs). Further information can be found at: https://docs.github.com/en/free-pro-team@latest/github/site-policy/</p>"},{"location":"imprint-privacy-policy/#google-analytics-4","title":"Google Analytics 4","text":"<p>If you have given your consent, Google Analytics 4, a web analysis service of Google LLC, is used on this website. The controller for users in the EU/EEA and Switzerland is Google Ireland Limited, Google Building Gordon House, 4 Barrow St, Dublin, D04 E5W5, Ireland (\"Google\").</p> <p>Google Analytics uses cookies that enable your use of our website to be analyzed. The information collected by the cookies about your use of this website is usually transferred to a Google server in the USA and stored there.</p> <p>In Google Analytics 4, the anonymization of IP addresses is activated by default. Due to IP anonymization, your IP address will be shortened by Google within member states of the European Union or in other contracting states of the Agreement on the European Economic Area. Only in exceptional cases will the full IP address be transmitted to a Google server in the USA and truncated there. According to Google, the IP address transmitted by your browser as part of Google Analytics will not be merged with other Google data. During your visit to the website, your user behavior is recorded in the form of \"events\". Events can be</p> <ul> <li>Page views</li> <li>First visit to the website</li> <li>Start of the session</li> <li>Websites visited</li> <li>Your \"click path\", interaction with the website</li> <li>Scrolls (whenever a user scrolls to the end of the page (90%))</li> <li>Clicks on external links</li> <li>Internal search queries</li> <li>Interaction with videos</li> <li>file downloads</li> <li>Viewed / clicked ads</li> <li>language setting</li> </ul> <p>Also recorded:</p> <ul> <li>Your approximate location (region)</li> <li>Date and time of your visit</li> <li>Your IP address (in abbreviated form)</li> <li>Technical information about your browser and the end devices you use (e.g. language setting, screen resolution)</li> <li>your internet provider</li> <li>the referrer URL (via which website/advertising medium you came to this website)</li> </ul> <p>On behalf of the operator of this website, Google will use this information to evaluate your use of the website and to compile reports on website activity. The reports provided by Google Analytics are used to analyze the performance of our website.</p> <p>Recipients of the data are/may be</p> <pre><code>Google Ireland Limited, Gordon House, Barrow Street, Dublin 4, Ireland (as processor pursuant to Art. 28 GDPR)\nGoogle LLC, 1600 Amphitheatre Parkway Mountain View, CA 94043, USA\nAlphabet Inc, 1600 Amphitheatre Parkway Mountain View, CA 94043, USA\n</code></pre> <p>For the USA, the European Commission adopted its adequacy decision on July 10, 2023. Google LLC is certified under the EU-US Privacy Framework. Since Google servers are distributed worldwide and a transfer to third countries (for example to Singapore) cannot be completely ruled out, we have also concluded the EU standard contractual clauses with the provider.</p> <p>The data sent by us and linked to cookies is automatically deleted after 2. The maximum lifespan of Google Analytics cookies is 2 years. Data whose retention period has been reached is automatically deleted once a month.</p> <p>The legal basis for this data processing is your consent in accordance with Art. 6 para. 1 sentence 1 lit. a GDPR and \u00a7 25 para. 1 sentence 1 TTDSG.</p> <p>You can also prevent the storage of cookies from the outset by setting your browser software accordingly. However, if you configure your browser to reject all cookies, this may limit the functionality of this and other websites. You can also prevent Google from collecting the data generated by the cookie and relating to your use of the website (including your IP address) and from processing this data by Google by</p> <p>You can find more information on the terms of use of Google Analytics and on data protection at Google at https://marketingplatform.google.com/about/analytics/terms/ and at https://policies.google.com</p>"},{"location":"security/","title":"Security","text":"<p>Swarm Learning operates on the premise of decentralized and secure machine learning. It's crucial to uphold the highest standards of security to maintain the integrity and confidentiality of your data. Here are the key security aspects to consider while leveraging the power of Swarm Learning:</p>"},{"location":"security/#1-raw-data-stays-on-devices","title":"1. Raw Data Stays on Devices:","text":"<p>In Swarm Learning, raw data remains decentralized on individual devices, ensuring data privacy and limiting access to authorized participants only.</p>"},{"location":"security/#2-peer-to-peer-device-connectivity","title":"2. Peer-to-Peer Device Connectivity:","text":"<p>Devices are connected peer-to-peer, establishing a collaborative learning environment. This approach enhances privacy by minimizing centralized data repositories.</p>"},{"location":"security/#3-encrypted-vpn-network","title":"3. Encrypted VPN Network:","text":"<p>The connection between devices is encrypted via a Virtual Private Network (VPN), adding an extra layer of security to data transmission, making it more resistant to unauthorized access.</p>"},{"location":"security/#4-certificate-based-participant-verification","title":"4. Certificate-based Participant Verification:","text":"<p>Only participants who have been pre-verified using certificates can actively participate in the Swarm Learning process. This stringent verification process ensures that only authenticated and authorized individuals are part of the training process.</p>"},{"location":"security/#vpn-structure","title":"VPN Structure","text":""},{"location":"security/#vpn-overview","title":"VPN overview","text":""},{"location":"security/#vpn-general-structure","title":"VPN general structure","text":"<p>Info</p> <p>Detailed guidelines and further insights into security practices with Swarm Learning, will follow.</p> <p>Bugs and Problems</p> <p>Did you find a bug in the code or other problems? Then raise an issue in our Github repository: https://github.com/KatherLab/swarm-learning-hpe/issues</p> <p>In case of problems or requests for improvement of the documentation, please raise an issue at: https://github.com/odelia-ai/odelia-ai.github.io/issues</p>"},{"location":"setup/","title":"Setup","text":"<p>Please replace the <code>&lt;placeholder&gt;</code> with the corresponding value:</p> <ul> <li><code>&lt;workspace_name&gt;</code>: Name of the folder in <code>/opt/hpe/swarm-learning-hpe/workspace</code> where the configuration files and model code is stored. e.g.: <code>odelia-breast-mri</code></li> <li><code>&lt;sentinel_ip&gt;</code>: IP address of the sentinel host. The sentinel host is the initiator of the network and the operator of the license server in our case. e.g.: <code>172.24.4.67</code></li> <li><code>&lt;host_index&gt;</code>: Name of the institution or partner participating in the training e.g.: <code>TUD</code></li> </ul>"},{"location":"setup/#installlation-of-hpe-swarm-learning","title":"Installlation of HPE Swarm Learning","text":"<p>Please only proceed to the next step by observing \"... is done successfully\" from the log.</p> <p>/opt/hpe/swarm-learning-hpe</p> <p>Please make sure you are in <code>/opt/hpe/swarm-learning-hpe</code> before you start the installation. You can get there with <code>cd /opt/hpe/swarm-learning-hpe</code>.</p>"},{"location":"setup/#1-prerequisite","title":"1. Prerequisite:","text":"<p>Runs scripts that check for required software and opens exposed ports.</p> <pre><code>sh workspace/automate_scripts/automate.sh -a\n</code></pre>"},{"location":"setup/#2-server-setup","title":"2. Server setup:","text":"<p>Runs scripts that sets up the swarm learning environment on a server.</p> <pre><code>sh workspace/automate_scripts/automate.sh -b -s &lt;sentinel_ip&gt; -d &lt;host_index&gt;\n</code></pre> <p>Credentials</p> <ul> <li>The script will ask for the HPE credentials. Please use your HPE Account credentials.</li> <li>The script will also ask for the VPN credentials. We use GoodAccess VPN. ODELIA consortium members get their credentials through the contact persons at TU Dresden. External users are welcome to open an issue on Github if they have any questions.</li> </ul>"},{"location":"setup/#3-final-setup","title":"3. Final setup:","text":"<p>Runs scripts that finalize the setup of the swarm learning environment.</p> <pre><code>sh workspace/automate_scripts/automate.sh -c -w &lt;workspace_name&gt; -s &lt;sentinel_ip&gt; -d &lt;host_index&gt; -l &lt;license_ip&gt; [-n num_peers] [-e num_epochs]\n</code></pre> <p>Optional flags</p> <ul> <li><code>-n num_peers</code>: Number of peers to be added to the network. e.g.: <code>3</code></li> <li><code>-e num_epochs</code>: Number of epochs to be trained. e.g.: <code>10</code></li> </ul>"},{"location":"setup/#upgrade-the-swarm-learning-environment","title":"Upgrade the Swarm Learning Environment","text":"<ol> <li> <p>If you already have the old version of Swarm learning installed. Run the following command to upgrade the Swarm Learning Environment from 1.x.x to 2.x.x <pre><code>sh workspace/automate_scripts/server_setup/cleanup_old_sl.sh\n</code></pre></p> </li> <li> <p>Then proceed to 1. Prerequisite in the installation guide.</p> </li> </ol> <p>Bugs and Problems</p> <p>Did you find a bug in the code or other problems? Then raise an issue in our Github repository: https://github.com/KatherLab/swarm-learning-hpe/issues</p> <p>In case of problems or requests for improvement of the documentation, please raise an issue at: https://github.com/odelia-ai/odelia-ai.github.io/issues</p>"},{"location":"usage/","title":"Usage","text":"<p>Please replace the <code>&lt;placeholder&gt;</code> with the corresponding value:</p> <ul> <li><code>&lt;workspace_name&gt;</code>: Name of the folder in <code>/opt/hpe/swarm-learning-hpe/workspace</code> where the configuration files and model code is stored. e.g.: <code>odelia-breast-mri</code></li> <li><code>&lt;sentinel_ip&gt;</code>: IP address of the sentinel host. The sentinel host is the initiator of the network and the operator of the license server in our case. e.g.: <code>172.24.4.67</code></li> <li><code>&lt;host_index&gt;</code>: Name of the institution or partner participating in the training e.g.: <code>TUD</code></li> </ul>"},{"location":"usage/#data-preparation","title":"Data Preparation","text":"<p>Place the data in the folder before you start training!</p> <ol> <li> <p>Create the folder structure for storing the data as follows:     <pre><code>cd /opt/hpe/swarm-learning-hpe/workspace/&lt;workspace_name&gt;\n</code></pre> <pre><code>mkdir user &amp;&amp; mkdir user/data-and-scratch &amp;&amp; mkdir user/data-and-scratch/data &amp;&amp; mkdir user/data-and-scratch/scratch &amp;&amp; chmod 777 -R /opt/hpe\n</code></pre> <pre><code>cd /opt/hpe/swarm-learning-hpe\n</code></pre></p> </li> <li> <p>Copy your Data to <code>opt/hpe/swarm-learning-hpe/workspace/&lt;workspace_name&gt;/user/data-and-scratch/data</code></p> </li> </ol>"},{"location":"usage/#running-swarm-learning-nodes","title":"Running Swarm Learning Nodes","text":"<p>Warning</p> <p>Enter root mode: <pre><code>sudo su\n</code></pre></p>"},{"location":"usage/#1-run-a-swarm-network-or-sentinel-node","title":"1. Run a Swarm Network (or Sentinel) node:","text":"<p>SN node</p> <p>SN nodes form the blockchain network. The current version of Swarm Learning uses an open-source version of Ethereum as the underlying blockchain platform. The SN nodes interact with each other using this blockchain platform to maintain and track progress. The SN nodes use this state and progress information to co-ordinate the working of the other swarm learning components. Sentinel Node is a special SN node. The Sentinel node is responsible for initializing the blockchain network. This is the first node to start.</p> <p>NOTE: Only metadata is written to the blockchain. The model itself is not stored in the blockchain.</p> <pre><code>./workspace/automate_scripts/launch_sl/run_sn.sh -s &lt;sentinel_ip&gt; -d &lt;host_index&gt;\n</code></pre>"},{"location":"usage/#2-run-a-swarm-swop-swarm-operator-node","title":"2. Run a Swarm SWOP (Swarm Operator) node:","text":"<p>SWOP</p> <p>SWOP is an agent that can manage Swarm Learning operations. SWOP is responsible to execute tasks that are assigned to it. A SWOP node can execute only one task at a time. SWOP helps in executing tasks such as starting and stopping Swarm runs, building and upgrading ML containers, and sharing models for training.</p> <pre><code>./workspace/automate_scripts/launch_sl/run_swop.sh -w &lt;workspace_name&gt; -s &lt;sentinel_ip&gt;  -d &lt;host_index&gt;\n</code></pre>"},{"location":"usage/#3-run-a-swarm-swci","title":"3. Run a Swarm SWCI","text":"<p>SWCI</p> <p>Swarm Learning Command Line Interface (SWCI) is the command interface tool to the Swarm Learning framework. It is used to view the status, control, and manage the Swarm Learning framework. SWCI manages the Swarm Learning framework using contexts and contracts.</p> <p>Warning</p> <p>SWCI node is used to generate training task runners, could be initiated by any host, but currently we suggest ONLY THE SENTINEL HOST IS ALLOWED TO INITIATE</p> <pre><code>./workspace/automate_scripts/launch_sl/run_swci.sh -w &lt;workspace_name&gt; -s &lt;sentinel_ip&gt;  -d &lt;host_index&gt;\n</code></pre>"},{"location":"usage/#results","title":"Results","text":"<p>Check the logs from training</p> <pre><code>./workspace/automate_scripts/launch_sl/check_latest_log.sh\n</code></pre> <p>View results under <code>workspace/&lt;workspace_name&gt;/user/data-and-scratch/scratch</code></p>"},{"location":"usage/#stop-swarm-learning","title":"Stop Swarm Learning","text":"<p>Stop and remove all Swarm Learning containers and volumes that are no longer needed with:  <pre><code>./workspace/swarm_learning_scripts/stop-swarm --[node_type]\n</code></pre></p> <p>Info</p> <p>--[node_type] is optional, if not specified, all the nodes will be stopped. Otherwise, specify --sn, --swop, --swci, --sl.</p> <p>Manually Remove containers and volumes</p> <ol> <li>List all Docker containers: <pre><code>docker ps -a\n</code></pre></li> <li>Remove all containers listed with docker ps -a <pre><code>docker rm &lt;container-id-1&gt; &lt;container-id-2&gt; &lt;container-id-n&gt; # Remove all listed with docker ps -a\n</code></pre></li> <li>List all Docker volumes: <pre><code>docker volume ls\n</code></pre></li> <li>Remove all volumes except sl-cli-lib <pre><code>docker volume rm &lt;volume-id&gt;\n</code></pre></li> </ol> <p>Bugs and Problems</p> <p>Did you find a bug in the code or other problems? Then raise an issue in our Github repository: https://github.com/KatherLab/swarm-learning-hpe/issues</p> <p>In case of problems or requests for improvement of the documentation, please raise an issue at: https://github.com/odelia-ai/odelia-ai.github.io/issues</p>"}]}